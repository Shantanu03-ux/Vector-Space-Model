{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b72af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Enter your query :  japan ruled korea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| Score | Document                       |\n",
      "------------------------------------------\n",
      "| 0.200 | D:/Shantanu/Docs\\T01.txt       |\n",
      "| 0.134 | D:/Shantanu/Docs\\T02.txt       |\n",
      "| 0.096 | D:/Shantanu/Docs\\T09.txt       |\n",
      "| 0.088 | D:/Shantanu/Docs\\T12.txt       |\n",
      "| 0.076 | D:/Shantanu/Docs\\T23.txt       |\n",
      "| 0.069 | D:/Shantanu/Docs\\T08.txt       |\n",
      "| 0.064 | D:/Shantanu/Docs\\T05.txt       |\n",
      "| 0.026 | D:/Shantanu/Docs\\T10.txt       |\n",
      "| 0.025 | D:/Shantanu/Docs\\T06.txt       |\n",
      "| 0.024 | D:/Shantanu/Docs\\T14.txt       |\n",
      "------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Enter your query :  japan korea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| Score | Document                       |\n",
      "------------------------------------------\n",
      "| 0.096 | D:/Shantanu/Docs\\T09.txt       |\n",
      "| 0.088 | D:/Shantanu/Docs\\T12.txt       |\n",
      "| 0.076 | D:/Shantanu/Docs\\T23.txt       |\n",
      "| 0.075 | D:/Shantanu/Docs\\T01.txt       |\n",
      "| 0.069 | D:/Shantanu/Docs\\T08.txt       |\n",
      "| 0.064 | D:/Shantanu/Docs\\T05.txt       |\n",
      "| 0.051 | D:/Shantanu/Docs\\T02.txt       |\n",
      "| 0.026 | D:/Shantanu/Docs\\T10.txt       |\n",
      "| 0.025 | D:/Shantanu/Docs\\T06.txt       |\n",
      "| 0.024 | D:/Shantanu/Docs\\T14.txt       |\n",
      "------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Enter your query :  hard-fought battles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| Score | Document                       |\n",
      "------------------------------------------\n",
      "| 0.105 | D:/Shantanu/Docs\\T35.txt       |\n",
      "| 0.087 | D:/Shantanu/Docs\\T31.txt       |\n",
      "| 0.079 | D:/Shantanu/Docs\\T37.txt       |\n",
      "| 0.068 | D:/Shantanu/Docs\\T26.txt       |\n",
      "| 0.065 | D:/Shantanu/Docs\\T18.txt       |\n",
      "------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Enter your query :  General Walker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| Score | Document                       |\n",
      "------------------------------------------\n",
      "| 0.078 | D:/Shantanu/Docs\\T16.txt       |\n",
      "| 0.076 | D:/Shantanu/Docs\\T35.txt       |\n",
      "| 0.064 | D:/Shantanu/Docs\\T13.txt       |\n",
      "| 0.058 | D:/Shantanu/Docs\\T38.txt       |\n",
      "| 0.055 | D:/Shantanu/Docs\\T33.txt       |\n",
      "| 0.053 | D:/Shantanu/Docs\\T14.txt       |\n",
      "| 0.051 | D:/Shantanu/Docs\\T21.txt       |\n",
      "| 0.050 | D:/Shantanu/Docs\\T15.txt       |\n",
      "| 0.049 | D:/Shantanu/Docs\\T17.txt       |\n",
      "| 0.049 | D:/Shantanu/Docs\\T20.txt       |\n",
      "------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Enter your query :  \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "CORPUS = \"D:/Shantanu/Docs/*\"\n",
    "\n",
    "\n",
    "# Each document has an id which are keys in the dictionary below.\n",
    "doc_files = dict()\n",
    "\n",
    "N = 0\n",
    "\n",
    "# vocabulary of the corpus\n",
    "vocabulary = set()\n",
    "\n",
    "# postings list is made which is a dict whose keys are the document ids of documents and the corresponding values as term frequency\n",
    "postings = defaultdict(dict)\n",
    "\n",
    "# document_frequency is a defaultdict whose keys are terms and corresponding values are number of documents\n",
    "document_frequency = defaultdict(int)\n",
    "\n",
    "length = defaultdict(float)\n",
    "\n",
    "   \n",
    "# To fetch the details about corpus\n",
    "def get_corpus():\n",
    "    global doc_files,N\n",
    "    documents = glob.glob(CORPUS)\n",
    "    N = len(documents)\n",
    "    doc_files = dict(zip(range(N), documents))\n",
    "\n",
    "    \n",
    "# Initialising terms and postings for the corpus   \n",
    "def inp():\n",
    "    global vocabulary,postings\n",
    "    for id in doc_files:\n",
    "        with open(doc_files[id], \"r\") as f:\n",
    "            document = f.read()\n",
    "        document = remove_special_characters(document)\n",
    "        document = remove_digits(document)\n",
    "        terms = tokenize(document)\n",
    "        unique_terms = set(terms)\n",
    "        vocabulary = vocabulary.union(unique_terms)\n",
    "        for term in unique_terms:\n",
    "            postings[term][id] = terms.count(term)\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    # using nltk tokenize library\n",
    "    terms = word_tokenize(document)\n",
    "    # Remove stopwords and convert terms to lowercase\n",
    "    terms = [term.lower() for term in terms if term not in STOPWORDS]\n",
    "    return terms\n",
    "\n",
    "# Set document frequencies for all the terms in vocabulary\n",
    "def indf():\n",
    "    global document_frequency\n",
    "    for term in vocabulary:\n",
    "        document_frequency[term] = len(postings[term])\n",
    "\n",
    "#calculate length for each doc for normalization\n",
    "def initialize_lengths():\n",
    "    global length\n",
    "    for id in doc_files:\n",
    "        l = 0\n",
    "        for term in vocabulary:\n",
    "            l += tf(term, id) ** 2\n",
    "        length[id] = math.sqrt(l)\n",
    "\n",
    "\n",
    "def tf(term, id):\n",
    "    # This gives the term frequency of term in document id using the function : 1 + log(tf).\n",
    "    # It returns 0 if the term isn't present in the document.\n",
    "    # The base of log is 10\n",
    "    if id in postings[term]:\n",
    "        return (1 + math.log(N/postings[term][id], 10))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def idf(term):\n",
    "    # Returns the inverse document frequency of term.\n",
    "    # The base of log is 10\n",
    "    if term in vocabulary:\n",
    "        return math.log(N / document_frequency[term], 10)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def print_scores(scores):\n",
    "    print(\"-\" * 42)\n",
    "    print(\"| %s | %-30s |\" % (\"Score\", \"Document\"))\n",
    "    print(\"-\" * 42)\n",
    "    c = 1\n",
    "    for (id, score) in scores:\n",
    "        if c>10:\n",
    "            break\n",
    "        if score != 0.0:\n",
    "            c+=1\n",
    "            print(\"| %s | %-30s |\" % (str(score)[:5], doc_files[id]))\n",
    "\n",
    "    print(\"-\" * 42, end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "def do_search():\n",
    "    query = tokenize(input(\" Enter your query : \"))\n",
    "\n",
    "    # Exit if query is empty\n",
    "    if query == []:\n",
    "        sys.exit()\n",
    "\n",
    "    scores = sorted(\n",
    "        [(id, cosine_similarity(query, id)) for id in range(N)],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def intersection(sets):\n",
    "    return reduce(set.intersection, [s for s in sets])\n",
    "\n",
    "\n",
    "def cosine_similarity(query, id):\n",
    "    \"\"\" \n",
    "    This gives the cosine_similarity between query and document id.\n",
    "    \"\"\"\n",
    "    cosine_similarity = 0.0\n",
    "\n",
    "    for term in query:\n",
    "        if term in vocabulary:\n",
    "            # calculate tf-idf score of the term and add to cosine_similarity\n",
    "            cosine_similarity += tf(term, id) * idf(term)\n",
    "\n",
    "    cosine_similarity = cosine_similarity / length[id]\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Removes special characters using regex  \n",
    "    regex = re.compile(r\"[^a-zA-Z0-9\\s]\")\n",
    "    return re.sub(regex, \"\", text)\n",
    "\n",
    "\n",
    "def remove_digits(text):\n",
    "    #Removes digits using regex \n",
    "    regex = re.compile(r\"\\d\")\n",
    "    return re.sub(regex, \"\", text)\n",
    "\n",
    "\n",
    "get_corpus()\n",
    "#initialize term and posting list\n",
    "inp()\n",
    "#intialize doc frequency\n",
    "indf()\n",
    "initialize_lengths()\n",
    "\n",
    "while True:\n",
    "    scores = do_search()\n",
    "    print_scores(scores)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "537e5f3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fc32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416b929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
